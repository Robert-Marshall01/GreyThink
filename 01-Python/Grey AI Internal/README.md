# Grey AI Internal

<div align="center">

**AI-Powered Data Insights Tool for Internal Operations**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688.svg)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18-61DAFB.svg)](https://reactjs.org)
[![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-black.svg)](https://ollama.ai)
[![License](https://img.shields.io/badge/License-Internal_Use-red.svg)](#)

*Upload CSV or text files â†’ Get AI-generated insights, summaries, and recommendations*

[Quick Start](#-quick-start) â€¢ [Architecture](#-architecture) â€¢ [API Reference](#-api-reference) â€¢ [Deployment](#-deployment) â€¢ [Testing](#-testing)

</div>

## WARNING:
There are known bugs, and the AI may be unreliable due to being self-hosted and open-source. Verify stability before deploying in a production environment.

---

## ğŸ“‹ Table of Contents

- [Why Grey AI Internal?](#-why-grey-ai-internal)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Usage Workflow](#-usage-workflow)
- [API Reference](#-api-reference)
- [AI Prompts & Examples](#-ai-prompts--examples)
- [Testing](#-testing)
- [Deployment](#-deployment)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Why Grey AI Internal?

Grey AI Internal demonstrates **production-ready engineering** for modern AI-integrated applications:

| Value Proposition | Description |
|-------------------|-------------|
| **Internal Tooling Focus** | Purpose-built for operations teams to extract insights from data without data science expertise |
| **No Paid APIs** | Uses local Ollama LLMâ€”zero API costs, full data privacy, works offline |
| **Senior Architecture** | Clean separation of concerns, async patterns, proper error handling, comprehensive testing |
| **Modern Stack** | FastAPI + React + PostgreSQLâ€”technologies in high demand across the industry |
| **Production-Ready** | Docker deployment, CI/CD pipeline, database migrations, environment configuration |

### Alignment with Industry Standards

This project showcases skills commonly requested in senior engineering roles:

- âœ… RESTful API design with FastAPI
- âœ… Async Python with SQLAlchemy 2.0
- âœ… React component architecture with Tailwind CSS
- âœ… LLM integration and prompt engineering
- âœ… Docker containerization and orchestration
- âœ… pytest testing with mocks and fixtures
- âœ… GitHub Actions CI/CD pipelines

---

## âœ¨ Features

- **File Upload** â€” Drag-and-drop CSV/TXT files with validation
- **Automatic Parsing** â€” Extract row counts, column metadata, statistics, and previews
- **AI Analysis** â€” Generate summaries, insights, and recommendations via local LLM
- **Custom Prompts** â€” Tailor analysis with specific instructions
- **Clean Dashboard** â€” Modern React UI with responsive design
- **Full Persistence** â€” All data and insights stored in database
- **API-First** â€” Complete REST API with Swagger documentation

---

## ğŸ›  Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GREY AI INTERNAL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend         â”‚  Backend          â”‚  AI Engine              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  React 18         â”‚  FastAPI          â”‚  Ollama                 â”‚
â”‚  Vite             â”‚  Python 3.11      â”‚  Llama 3 / Mistral      â”‚
â”‚  Tailwind CSS     â”‚  SQLAlchemy 2.0   â”‚  Local Inference        â”‚
â”‚  Axios            â”‚  Pydantic v2      â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Database         â”‚  DevOps           â”‚  Testing                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  SQLite (dev)     â”‚  Docker           â”‚  pytest                 â”‚
â”‚  PostgreSQL       â”‚  Docker Compose   â”‚  pytest-asyncio         â”‚
â”‚  Alembic          â”‚  GitHub Actions   â”‚  httpx (async client)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ— Architecture

### System Diagram

```mermaid
flowchart TB
    subgraph Frontend["Frontend (React + Tailwind)"]
        UI[Dashboard UI]
        Upload[File Upload]
        Display[Insights Display]
    end

    subgraph Backend["Backend (FastAPI)"]
        API[REST API]
        Parser[Parser Service]
        AI[AI Service]
        Persist[Persistence Service]
    end

    subgraph Storage["Data Layer"]
        DB[(PostgreSQL/SQLite)]
        Files[File Storage]
    end

    subgraph LLM["AI Engine"]
        Ollama[Ollama Server]
        Model[Llama 3 / Mistral]
    end

    UI --> API
    Upload --> API
    API --> Parser
    API --> AI
    API --> Persist
    Parser --> Files
    Persist --> DB
    AI --> Ollama
    Ollama --> Model
    Display --> API
```

### ASCII Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER INTERFACE                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         React Dashboard                              â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚ File Upload  â”‚   â”‚ Metrics View  â”‚   â”‚  Insights Panel    â”‚    â”‚    â”‚
â”‚  â”‚   â”‚ (drag/drop)  â”‚   â”‚ (stats/chart) â”‚   â”‚  (AI summaries)    â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ HTTP/REST (JSON)
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              BACKEND API                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                          FastAPI Server                              â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Routes Layer      â†’    Services Layer     â†’    Data Layer         â”‚    â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚    â”‚
â”‚  â”‚   /api/upload/           ParserService          SQLAlchemy ORM      â”‚    â”‚
â”‚  â”‚   /api/ai/               AIService               â†“                  â”‚    â”‚
â”‚  â”‚   /health                PersistenceService      PostgreSQL/SQLite  â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP (async)
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      OLLAMA SERVER        â”‚
                    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚   â”‚    Local LLM      â”‚   â”‚
                    â”‚   â”‚  (llama3/mistral) â”‚   â”‚
                    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚   localhost:11434         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Upload  â”‚ â”€â”€â–¶ â”‚  Parse   â”‚ â”€â”€â–¶ â”‚  Store   â”‚ â”€â”€â–¶ â”‚ Analyze  â”‚ â”€â”€â–¶ â”‚ Display  â”‚
â”‚   File   â”‚     â”‚  Data    â”‚     â”‚ Metrics  â”‚     â”‚  (AI)    â”‚     â”‚ Insights â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚                â”‚                â”‚                â”‚
     â–¼                â–¼                â–¼                â–¼                â–¼
  Validate        Extract          Upload +        Call Ollama      Return JSON
  Extension      Statistics       Metrics DB       with prompt       to frontend
  & Size                          records                          
```

### Database Schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    uploads     â”‚       â”‚     metrics     â”‚       â”‚   ai_outputs   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)        â”‚â”€â”€â”    â”‚ id (PK)         â”‚â”€â”€â”    â”‚ id (PK)        â”‚
â”‚ filename       â”‚  â”‚    â”‚ upload_id (FK)  â”‚â—€â”€â”˜    â”‚ upload_id (FK) â”‚â—€â”€â”
â”‚ file_path      â”‚  â”‚    â”‚ row_count       â”‚       â”‚ ai_summary     â”‚  â”‚
â”‚ file_type      â”‚  â”‚    â”‚ column_count    â”‚       â”‚ insights_json  â”‚  â”‚
â”‚ file_size      â”‚  â””â”€â”€â”€â–¶â”‚ columns_json    â”‚       â”‚ ai_recommend.  â”‚  â”‚
â”‚ status         â”‚       â”‚ stats_json      â”‚       â”‚ model_used     â”‚  â”‚
â”‚ created_at     â”‚       â”‚ preview_json    â”‚       â”‚ created_at     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                  â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            1:1 relationships
```

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| Python | 3.11+ | Backend runtime |
| Node.js | 18+ | Frontend build |
| Ollama | Latest | Local LLM inference |
| Docker | 20+ | Containerization (optional) |

### Option 1: Docker (Recommended)

The fastest way to run everything:

```bash
# 1. Clone and enter project
cd grey-ai-internal

# 2. Copy environment config
cp .env.example .env

# 3. Start all services
docker-compose up -d --build

# 4. Pull an LLM model (first run only)
docker exec -it grey-ai-ollama ollama pull llama3

# 5. Access the application
#    Frontend:  http://localhost:3000
#    Backend:   http://localhost:8000
#    API Docs:  http://localhost:8000/docs
```

### Option 2: Manual Setup

#### Step 1: Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull llama3    # Recommended (~8GB)
ollama pull mistral   # Alternative (~4GB)
ollama pull phi3      # Lightweight (~2GB)

# Start Ollama server
ollama serve
```

#### Step 2: Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env as needed

# Run database migrations
# Tables are auto-created on first run

# Start the server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

#### Step 3: Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

### Access Points

| Service | URL | Description |
|---------|-----|-------------|
| Frontend | http://localhost:5173 | React dashboard |
| Backend API | http://localhost:8000 | FastAPI server |
| API Documentation | http://localhost:8000/docs | Swagger UI |
| Ollama | http://localhost:11434 | LLM service |

---

## ğŸ“Š Usage Workflow

### Step 1: Upload Data

Upload a CSV or text file through the dashboard or API:

```bash
curl -X POST "http://localhost:8000/api/upload/" \
  -F "file=@sales_data.csv"
```

**Response:**
```json
{
  "id": 1,
  "filename": "sales_data.csv",
  "file_type": "csv",
  "file_size": 102400,
  "status": "parsed",
  "row_count": 1500,
  "column_count": 8,
  "has_metrics": true,
  "has_insights": false,
  "message": "File uploaded and parsed successfully"
}
```

### Step 2: View Parsed Metrics

The system automatically extracts:
- Row and column counts
- Column names and data types
- Statistical summaries (min, max, mean, median)
- Preview of first 10 rows

```bash
curl "http://localhost:8000/api/upload/1"
```

### Step 3: Trigger AI Analysis

Request AI-generated insights:

```bash
curl -X POST "http://localhost:8000/api/ai/analyze/1" \
  -H "Content-Type: application/json" \
  -d '{"custom_prompt": "Focus on revenue trends and top performers"}'
```

**Response:**
```json
{
  "id": 1,
  "upload_id": 1,
  "summary": "The sales data reveals strong Q4 performance with a 15% year-over-year increase. The North region leads with 23% market share, while Product A shows consistent month-over-month growth.",
  "insights": [
    "Revenue peaked in December at $120K, representing 18% of annual total",
    "North region outperformed others by 23% on average",
    "Product A maintained 12% growth rate throughout the year",
    "Customer retention improved from 72% to 78%",
    "Seasonal patterns indicate Q4 accounts for 35% of annual revenue"
  ],
  "recommendations": {
    "actions": [
      "Increase inventory for Product A before Q4 2026",
      "Expand sales team in underperforming West region",
      "Investigate customer churn in South region",
      "Develop Q1 promotional strategy to reduce seasonal variance"
    ],
    "priority": "high"
  },
  "model_used": "llama3",
  "created_at": "2026-01-30T10:35:00Z"
}
```

### Step 4: View Insights in Dashboard

The React dashboard displays:
- Executive summary at a glance
- Key findings with bullet points
- Actionable recommendations
- Option to re-analyze with different prompts

---

## ğŸ“¡ API Reference

### Endpoints Overview

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/api/upload/` | Upload file |
| `GET` | `/api/upload/` | List uploads (paginated) |
| `GET` | `/api/upload/{id}` | Get upload details |
| `DELETE` | `/api/upload/{id}` | Delete upload |
| `POST` | `/api/ai/analyze/{id}` | Trigger AI analysis |
| `GET` | `/api/ai/insights/{id}` | Get stored insights |
| `POST` | `/api/ai/infer` | Raw inference |
| `GET` | `/api/ai/status` | AI service status |

### Example Requests

<details>
<summary><strong>Upload a File</strong></summary>

```bash
curl -X POST "http://localhost:8000/api/upload/" \
  -F "file=@quarterly_sales.csv"
```

Response (201 Created):
```json
{
  "id": 1,
  "filename": "quarterly_sales.csv",
  "file_type": "csv",
  "file_size": 45678,
  "status": "parsed",
  "row_count": 250,
  "column_count": 12,
  "created_at": "2026-01-30T10:30:00Z",
  "has_metrics": true,
  "has_insights": false
}
```
</details>

<details>
<summary><strong>List Uploads with Pagination</strong></summary>

```bash
curl "http://localhost:8000/api/upload/?page=1&page_size=10&status_filter=analyzed"
```

Response:
```json
{
  "items": [
    {
      "id": 2,
      "filename": "revenue_2025.csv",
      "file_type": "csv",
      "status": "analyzed",
      "row_count": 1200,
      "has_insights": true,
      "created_at": "2026-01-29T14:22:00Z"
    }
  ],
  "total": 15,
  "page": 1,
  "page_size": 10,
  "has_more": true
}
```
</details>

<details>
<summary><strong>Trigger AI Analysis</strong></summary>

```bash
curl -X POST "http://localhost:8000/api/ai/analyze/1" \
  -H "Content-Type: application/json" \
  -d '{"custom_prompt": "Identify anomalies and unusual patterns"}'
```

Response:
```json
{
  "id": 1,
  "upload_id": 1,
  "summary": "Analysis reveals several data anomalies...",
  "insights": ["Finding 1", "Finding 2"],
  "recommendations": {"actions": ["Action 1"]},
  "model_used": "llama3",
  "created_at": "2026-01-30T10:35:00Z"
}
```
</details>

<details>
<summary><strong>Check AI Service Status</strong></summary>

```bash
curl "http://localhost:8000/api/ai/status"
```

Response:
```json
{
  "available": true,
  "configured_model": "llama3",
  "ollama_url": "http://localhost:11434",
  "available_models": ["llama3", "mistral", "phi3"],
  "status": "healthy"
}
```
</details>

---

## ğŸ¤– AI Prompts & Examples

Grey AI Internal uses structured prompts optimized for data analysis. Here are the built-in prompt templates:

### System Prompts

| Mode | Purpose | Use Case |
|------|---------|----------|
| `default` | Balanced analysis | General data review |
| `trends` | Temporal patterns | Time-series data |
| `anomalies` | Outlier detection | Quality assurance |
| `optimization` | Efficiency improvements | Operations analysis |

### Example Prompts

<details>
<summary><strong>Summarize Trends</strong></summary>

```
Analyze the dataset and identify key trends:
1. What are the main patterns over time?
2. Are there any seasonal variations?
3. What is the overall direction (growth/decline)?
4. Which categories/segments show the strongest trends?
```
</details>

<details>
<summary><strong>Detect Anomalies</strong></summary>

```
Examine the data for anomalies and outliers:
1. Identify any values that deviate significantly from the norm
2. Look for unexpected patterns or breaks in continuity
3. Highlight potential data quality issues
4. Suggest which anomalies warrant further investigation
```
</details>

<details>
<summary><strong>Suggest Optimizations</strong></summary>

```
Based on the data, suggest optimizations:
1. What inefficiencies can you identify?
2. Where are the biggest opportunities for improvement?
3. What actions would have the highest impact?
4. Prioritize recommendations by effort vs. impact
```
</details>

<details>
<summary><strong>Executive Summary</strong></summary>

```
Provide an executive summary suitable for stakeholders:
1. Key metrics and their current state
2. Most important findings (positive and negative)
3. Critical action items
4. Recommended next steps
```
</details>

### Custom Prompt Example

```bash
curl -X POST "http://localhost:8000/api/ai/analyze/1" \
  -H "Content-Type: application/json" \
  -d '{
    "custom_prompt": "Compare Q4 performance against Q3. Focus on:
      1. Revenue changes by region
      2. Product category growth rates
      3. Customer acquisition vs retention
      4. Recommend budget allocation for Q1"
  }'
```

---

## ğŸ§ª Testing

### Running Tests

```bash
cd backend

# Run all tests
pytest

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/unit/test_parser.py -v

# Run with coverage
pytest --cov=. --cov-report=term-missing

# Run only unit tests
pytest tests/unit

# Run only integration tests
pytest tests/integration
```

### Test Structure

```
backend/tests/
â”œâ”€â”€ conftest.py                 # Shared fixtures
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_parser.py          # Parser service tests
â”‚   â”œâ”€â”€ test_ai_service.py      # AI service tests (mocked)
â”‚   â””â”€â”€ test_models.py          # Database model tests
â””â”€â”€ integration/
    â””â”€â”€ test_api.py             # API endpoint tests
```

### Key Test Features

| Feature | Description |
|---------|-------------|
| **Async fixtures** | pytest-asyncio for async test support |
| **In-memory DB** | SQLite `:memory:` for fast, isolated tests |
| **Mocked Ollama** | HTTP calls mocked to avoid real inference |
| **Sample data** | CSV/text fixtures for consistent testing |
| **Coverage reporting** | Track test coverage with pytest-cov |

### GitHub Actions CI

Tests run automatically on every push and pull request:

```yaml
# .github/workflows/tests.yml
- Lint backend code (ruff, black)
- Run unit tests
- Run integration tests
- Generate coverage report
- Build Docker images
```

---

## ğŸš¢ Deployment

### Docker Compose (Recommended)

```bash
# Start all services
docker-compose up -d --build

# Pull LLM model
docker exec -it grey-ai-ollama ollama pull llama3

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Reset (including data)
docker-compose down -v
```

### Services in Docker

| Service | Container | Port |
|---------|-----------|------|
| Frontend | grey-ai-frontend | 3000 |
| Backend | grey-ai-backend | 8000 |
| Database | grey-ai-db | 5432 |
| Ollama | grey-ai-ollama | 11434 |

### Production Considerations

1. **Database**: Switch to PostgreSQL (configured in Docker)
2. **HTTPS**: Add nginx reverse proxy with SSL
3. **Environment**: Set `DEBUG=false` in production
4. **Secrets**: Use Docker secrets or environment injection
5. **GPU**: Enable NVIDIA GPU support for faster inference

### Manual Production Deployment

```bash
# Backend (with gunicorn)
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000

# Frontend (build and serve)
npm run build
# Serve dist/ with nginx
```

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | Database connection string | `sqlite+aiosqlite:///./data.db` |
| `OLLAMA_BASE_URL` | Ollama server URL | `http://localhost:11434` |
| `OLLAMA_MODEL` | LLM model to use | `llama3` |
| `OLLAMA_TIMEOUT` | Request timeout (seconds) | `120` |
| `MAX_FILE_SIZE_MB` | Upload size limit | `10` |
| `ALLOWED_EXTENSIONS` | Allowed file types | `.csv,.txt` |
| `CORS_ORIGINS` | Allowed frontend origins | `http://localhost:5173` |
| `DEBUG` | Enable debug mode | `true` |

### Switching LLM Models

```bash
# Pull new model
ollama pull mistral

# Update .env
OLLAMA_MODEL=mistral

# Restart backend
docker-compose restart backend
```

**Recommended Models:**

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama3 | ~8GB | Medium | Excellent | General analysis |
| mistral | ~4GB | Fast | Good | Balanced workloads |
| phi3 | ~2GB | Fastest | Acceptable | Quick insights |

---

## ğŸ”§ Troubleshooting

### Common Issues

<details>
<summary><strong>"Cannot connect to Ollama"</strong></summary>

1. Ensure Ollama is running: `ollama serve`
2. Check URL in `.env`: `OLLAMA_BASE_URL=http://localhost:11434`
3. For Docker: use `http://ollama:11434` (internal network)
</details>

<details>
<summary><strong>"Model not found"</strong></summary>

1. Pull the model: `ollama pull llama3`
2. Check model name in `.env`: `OLLAMA_MODEL=llama3`
3. List available models: `ollama list`
</details>

<details>
<summary><strong>"File upload fails"</strong></summary>

1. Check file size (default max: 10MB)
2. Ensure file is `.csv` or `.txt`
3. Check disk space in uploads directory
</details>

<details>
<summary><strong>"Slow AI analysis"</strong></summary>

1. Use a smaller model: `phi3`
2. Reduce data size before upload
3. Enable GPU acceleration (NVIDIA)
4. Increase `OLLAMA_TIMEOUT` if needed
</details>

<details>
<summary><strong>"Database errors"</strong></summary>

1. Check `DATABASE_URL` in `.env`
2. Ensure database container is running
3. Reset: `docker-compose down -v && docker-compose up -d`
</details>

---

## ğŸ“ Project Structure

```
grey-ai-internal/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile              # Backend container
â”‚   â”œâ”€â”€ pytest.ini              # Test configuration
â”‚   â”œâ”€â”€ .env.example            # Environment template
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ upload.py           # File upload routes
â”‚   â”‚   â””â”€â”€ ai.py               # AI inference routes
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”œâ”€â”€ parser.py           # CSV/text parsing
â”‚   â”‚   â”œâ”€â”€ ai_service.py       # Ollama integration
â”‚   â”‚   â””â”€â”€ persistence.py      # Database operations
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â”œâ”€â”€ database.py         # SQLAlchemy ORM
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic schemas
â”‚   â”‚
â”‚   â””â”€â”€ tests/                  # Test suite
â”‚       â”œâ”€â”€ unit/               # Unit tests
â”‚       â””â”€â”€ integration/        # API tests
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main application
â”‚   â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â””â”€â”€ services/           # API client
â”‚   â”œâ”€â”€ Dockerfile              # Frontend container
â”‚   â”œâ”€â”€ nginx.conf              # Production server
â”‚   â””â”€â”€ package.json            # Node dependencies
â”‚
â”œâ”€â”€ docker-compose.yml          # Multi-container orchestration
â”œâ”€â”€ .env.example                # Root environment template
â”œâ”€â”€ .github/workflows/          # CI/CD pipeline
â”‚   â””â”€â”€ tests.yml               # Automated testing
â”‚
â”œâ”€â”€ docs/                       # Additional documentation
â”‚   â””â”€â”€ API_EXAMPLES.md         # API usage examples
â”‚
â”œâ”€â”€ ARCHITECTURE.md             # Technical architecture
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“„ License

Internal use only. All rights reserved.

---

<div align="center">

**Built with â¤ï¸ for data-driven teams**

*Grey AI Internal â€” Turn your data into actionable insights*

</div>
