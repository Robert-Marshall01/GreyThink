// GreyStd Test - Bench
// Benchmarking utilities for performance measurement

// Create a benchmark
fn Benchmark_new(name, bench_fn) {
  {
    _type: "Benchmark",
    name: name,
    bench_fn: bench_fn,
    iterations: 100,
    warmup: 10
  }
}

// Set iteration count
fn bench_iterations(bench, n) {
  merge(bench, { iterations: n })
}

// Set warmup count
fn bench_warmup(bench, n) {
  merge(bench, { warmup: n })
}

// Run a benchmark
fn run_benchmark(bench) {
  fn f() { get(bench, "bench_fn") }
  fn iterations() { get(bench, "iterations") }
  fn warmup_count() { get(bench, "warmup") }

  // Warmup phase
  forEach(range(0, warmup_count()), fn(_) { f()() })

  // Measurement phase
  fn times() {
    map(range(0, iterations()), fn(_) {
      fn start() { timestamp() }
      f()()
      sub(timestamp(), start())
    })
  }

  fn sorted() { sort(times()) }
  fn total() { reduce(sorted(), 0, fn(acc, t) { add(acc, t) }) }
  fn avg() { div(total(), iterations()) }
  fn min_t() { get(sorted(), 0) }
  fn max_t() { get(sorted(), sub(length(sorted()), 1)) }
  fn median_t() { get(sorted(), floor(div(length(sorted()), 2))) }
  fn p95_idx() { floor(mul(length(sorted()), 0.95)) }
  fn p95() { get(sorted(), p95_idx()) }

  {
    _type: "BenchResult",
    name: get(bench, "name"),
    iterations: iterations(),
    total_ms: total(),
    avg_ms: avg(),
    min_ms: min_t(),
    max_ms: max_t(),
    median_ms: median_t(),
    p95_ms: p95(),
    ops_per_sec: if_then(gt(avg(), 0), fn() { div(1000, avg()) }, fn() { 0 })
  }
}

// Print benchmark results
fn print_bench_result(br) {
  fn round2(n) { div(round(mul(n, 100)), 100) }
  println(concat_str("\nBenchmark: ", get(br, "name")))
  println(concat_str("  Iterations: ", to_string(get(br, "iterations"))))
  println(concat_str("  Avg: ", to_string(round2(get(br, "avg_ms"))), "ms"))
  println(concat_str("  Min: ", to_string(round2(get(br, "min_ms"))), "ms"))
  println(concat_str("  Max: ", to_string(round2(get(br, "max_ms"))), "ms"))
  println(concat_str("  Median: ", to_string(round2(get(br, "median_ms"))), "ms"))
  println(concat_str("  P95: ", to_string(round2(get(br, "p95_ms"))), "ms"))
  println(concat_str("  Ops/sec: ", to_string(round(get(br, "ops_per_sec")))))
}

// Create a benchmark suite
fn BenchSuite_new(name) {
  { _type: "BenchSuite", name: name, benchmarks: [] }
}

// Add bench to suite
fn benchsuite_add(suite, bench) {
  merge(suite, { benchmarks: append(get(suite, "benchmarks"), bench) })
}

// Run all benchmarks in suite
fn run_benchsuite(suite) {
  println(concat_str("\n=== Benchmark Suite: ", get(suite, "name"), " ==="))
  fn results() {
    map(get(suite, "benchmarks"), fn(b) {
      fn result() { run_benchmark(b) }
      print_bench_result(result())
      result()
    })
  }
  {
    _type: "BenchSuiteResult",
    suite: get(suite, "name"),
    results: results(),
    count: length(results())
  }
}

// Compare two benchmark results
fn bench_compare(a, b) {
  fn ratio() { div(get(a, "avg_ms"), get(b, "avg_ms")) }
  fn faster() { if_then(lt(ratio(), 1), fn() { get(a, "name") }, fn() { get(b, "name") }) }
  fn speedup() { if_then(lt(ratio(), 1), fn() { div(1, ratio()) }, fn() { ratio() }) }
  {
    a: get(a, "name"),
    b: get(b, "name"),
    ratio: ratio(),
    faster: faster(),
    speedup: concat_str(to_string(div(round(mul(speedup(), 100)), 100)), "x")
  }
}
