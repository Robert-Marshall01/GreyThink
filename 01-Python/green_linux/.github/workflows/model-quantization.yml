name: Model quantization

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  quantize-model:
    runs-on: ubuntu-latest
    permissions: read-all
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.12
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install scikit-learn joblib skl2onnx onnx onnxruntime onnxconverter-common pytest
      - name: Run quantization flow and tests
        env:
          MIN_DELTA_NDCG: '0.01'
          MIN_DELTA_TOP1: '0.00'
        run: |
          python scripts/train.py --out models/window_predictor.joblib --n-series 100 --n-estimators 5
          python scripts/convert_to_onnx.py || true
          python scripts/fix_onnx_opset.py || true
          # try quantize dynamically (will succeed after opset fix)
          python - << 'PY'
import sys
try:
    from onnxruntime.quantization import quantize_dynamic, QuantType
    import pathlib
    inf = pathlib.Path('models') / 'window_predictor.opsetfixed.onnx'
    if inf.exists():
        quantize_dynamic(str(inf), str(pathlib.Path('models') / 'window_predictor.opsetfixed.quant.onnx'), weight_type=QuantType.QInt8)
except Exception as e:
    print('Quantize step warning:', e)
    pass
PY
          python -m pytest tests/test_onnx_model.py::test_onnx_model_exists_and_size -q
          # generate calibration data for static quantization
          mkdir -p evaluation
          python scripts/generate_dataset.py --n-series 200 --length 48 --window-hours 2 --calibration-out evaluation/calibration.npz --calibration-samples 512 --seed 12345
          # attempt static quantization using the calibration file (graceful fallback)
          python scripts/quantize_static.py --calibration evaluation/calibration.npz || true
          # run the CI quantization tests
          python -m pytest tests/test_quantization_ci.py -q
          # run evaluation with small dataset and check model quality thresholds
          python -m pytest tests/test_evaluation_metrics.py -q
      - name: Run evaluation and save report
        run: |
          mkdir -p evaluation
          python scripts/evaluate_model.py --n-series 200 --length 48 --window-hours 2 --n-estimators 20 --report-out evaluation/report.json || true
      - name: Upload evaluation report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-report
          path: evaluation/report.json
      - name: Upload model artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: models/**

      - name: Upload calibration artifact
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: calibration-file
          path: evaluation/calibration.npz

      - name: Annotate evaluation report on failure
        if: failure()
        run: |
          python - << 'PY'
import json, sys
p='evaluation/report.json'
try:
    with open(p) as f:
        r = json.load(f)
    m = r.get('metrics', {})
    metrics_str = ' '.join(f"{k}={v}" for k, v in m.items())
    # Create an error annotation pointing to the report file and a notice with full report
    print(f"::error file={p}::Evaluation gating failed. Metrics: {metrics_str}")
    print(f"::notice file={p}::Full report: {json.dumps(r)}")
except Exception as e:
    print(f"::warning::Could not read evaluation report: {e}")
    sys.exit(0)
PY

      - name: Add evaluation report to job summary
        if: always()
        run: |
          if [ -f evaluation/report.json ]; then
            python - << 'PY'
import json, os
p='evaluation/report.json'
with open(p) as f:
    r=json.load(f)
m=r.get('metrics', {})
lines=["# Evaluation Report", ""]
lines.append("## Metrics")
for k,v in m.items():
    lines.append(f"- **{k}**: {v}")
lines.extend(["", "**Full report JSON:**", "```json", json.dumps(r, indent=2), "```"])
summary_path=os.environ.get('GITHUB_STEP_SUMMARY', '/tmp/step_summary')
with open(summary_path, 'a') as fh:
    fh.write('\n'.join(lines))
print('Wrote evaluation report to job summary')
PY
          else
            echo "No evaluation report found to add to job summary" >> $GITHUB_STEP_SUMMARY
          fi
