/// io::mmap — Memory-mapped file I/O.

use crate::core::option::Option;
use crate::io::{IoResult, IoError, IoErrorKind};
use crate::io::file::File;

/// Options for creating a memory map.
///
/// ```grey
/// let file = File::open("large_data.bin")?;
/// let mmap = MmapOptions::new()
///     .offset(0)
///     .len(file.len()? as usize)
///     .map(&file)?;
///
/// // Access file contents as a byte slice — zero copy!
/// let header = &mmap[..4];
/// ```
pub struct MmapOptions {
    offset: u64,
    len: Option<usize>,
    populate: bool,
    huge_pages: bool,
}

impl MmapOptions {
    pub fn new() -> MmapOptions {
        MmapOptions {
            offset: 0,
            len: None,
            populate: false,
            huge_pages: false,
        }
    }

    /// Sets the offset within the file to start the mapping.
    pub fn offset(mut self, offset: u64) -> &mut MmapOptions {
        self.offset = offset;
        self
    }

    /// Sets the length of the mapping. Defaults to file length minus offset.
    pub fn len(mut self, len: usize) -> &mut MmapOptions {
        self.len = Some(len);
        self
    }

    /// Pre-populates the page tables for the mapping.
    pub fn populate(mut self) -> &mut MmapOptions {
        self.populate = true;
        self
    }

    /// Requests huge pages for the mapping (Linux-specific).
    pub fn huge_pages(mut self) -> &mut MmapOptions {
        self.huge_pages = true;
        self
    }

    /// Creates a read-only memory map of the file.
    pub fn map(self, file: &File) -> IoResult<Mmap> {
        let length = match self.len {
            Some(l) => l,
            None => {
                let file_len = file.len()?;
                (file_len - self.offset) as usize
            }
        };

        if length == 0 {
            return Ok(Mmap { ptr: core::ptr::null(), len: 0 });
        }

        let ptr = intrinsics::mmap(
            core::ptr::null_mut(),
            length,
            PROT_READ,
            MAP_SHARED,
            file.as_raw_fd(),
            self.offset as i64,
        )?;

        if self.populate {
            intrinsics::madvise(ptr, length, MADV_WILLNEED)?;
        }

        Ok(Mmap { ptr: ptr as *const u8, len: length })
    }

    /// Creates a mutable memory map of the file.
    pub fn map_mut(self, file: &File) -> IoResult<MmapMut> {
        let length = match self.len {
            Some(l) => l,
            None => {
                let file_len = file.len()?;
                (file_len - self.offset) as usize
            }
        };

        if length == 0 {
            return Ok(MmapMut { ptr: core::ptr::null_mut(), len: 0 });
        }

        let ptr = intrinsics::mmap(
            core::ptr::null_mut(),
            length,
            PROT_READ | PROT_WRITE,
            MAP_SHARED,
            file.as_raw_fd(),
            self.offset as i64,
        )?;

        Ok(MmapMut { ptr: ptr as *mut u8, len: length })
    }

    /// Creates an anonymous (not file-backed) mutable memory map.
    pub fn map_anon(self, len: usize) -> IoResult<MmapMut> {
        let ptr = intrinsics::mmap(
            core::ptr::null_mut(),
            len,
            PROT_READ | PROT_WRITE,
            MAP_PRIVATE | MAP_ANONYMOUS,
            -1,
            0,
        )?;

        Ok(MmapMut { ptr: ptr as *mut u8, len })
    }
}

/// A read-only memory-mapped region.
pub struct Mmap {
    ptr: *const u8,
    len: usize,
}

impl Mmap {
    /// Returns the mapped data as a byte slice.
    pub fn as_slice(self) -> &[u8] {
        if self.ptr.is_null() {
            &[]
        } else {
            unsafe { core::slice::from_raw_parts(self.ptr, self.len) }
        }
    }

    /// Returns the length of the mapping.
    pub fn len(self) -> usize {
        self.len
    }

    /// Returns true if the mapping is empty.
    pub fn is_empty(self) -> bool {
        self.len == 0
    }

    /// Advise the kernel about access patterns.
    pub fn advise(self, advice: MmapAdvice) -> IoResult<()> {
        if !self.ptr.is_null() {
            intrinsics::madvise(self.ptr as *mut u8, self.len, advice.to_raw())
        } else {
            Ok(())
        }
    }

    /// Makes the mapping mutable.
    pub fn make_mut(self) -> IoResult<MmapMut> {
        if self.ptr.is_null() {
            return Ok(MmapMut { ptr: core::ptr::null_mut(), len: 0 });
        }
        intrinsics::mprotect(self.ptr as *mut u8, self.len, PROT_READ | PROT_WRITE)?;
        let ptr = self.ptr as *mut u8;
        let len = self.len;
        core::mem::forget(self); // prevent double unmap
        Ok(MmapMut { ptr, len })
    }
}

impl Index<RangeFull> for Mmap {
    type Output = [u8];
    fn index(self, _: RangeFull) -> &[u8] { self.as_slice() }
}

impl Index<Range<usize>> for Mmap {
    type Output = [u8];
    fn index(self, range: Range<usize>) -> &[u8] {
        &self.as_slice()[range]
    }
}

impl Drop for Mmap {
    fn drop(mut self) {
        if !self.ptr.is_null() {
            let _ = intrinsics::munmap(self.ptr as *mut u8, self.len);
        }
    }
}

// Mmap is safe to send across threads (read-only)
unsafe impl Send for Mmap {}
unsafe impl Sync for Mmap {}

/// A mutable memory-mapped region.
pub struct MmapMut {
    ptr: *mut u8,
    len: usize,
}

impl MmapMut {
    /// Returns the mapped data as a byte slice.
    pub fn as_slice(self) -> &[u8] {
        if self.ptr.is_null() {
            &[]
        } else {
            unsafe { core::slice::from_raw_parts(self.ptr, self.len) }
        }
    }

    /// Returns the mapped data as a mutable byte slice.
    pub fn as_mut_slice(mut self) -> &mut [u8] {
        if self.ptr.is_null() {
            &mut []
        } else {
            unsafe { core::slice::from_raw_parts_mut(self.ptr, self.len) }
        }
    }

    /// Returns the length of the mapping.
    pub fn len(self) -> usize {
        self.len
    }

    /// Flushes outstanding changes to disk synchronously.
    pub fn flush(self) -> IoResult<()> {
        if !self.ptr.is_null() {
            intrinsics::msync(self.ptr, self.len, MS_SYNC)
        } else {
            Ok(())
        }
    }

    /// Flushes outstanding changes to disk asynchronously.
    pub fn flush_async(self) -> IoResult<()> {
        if !self.ptr.is_null() {
            intrinsics::msync(self.ptr, self.len, MS_ASYNC)
        } else {
            Ok(())
        }
    }

    /// Flushes a specific byte range.
    pub fn flush_range(self, offset: usize, len: usize) -> IoResult<()> {
        if offset + len > self.len {
            return Err(IoError::new(IoErrorKind::InvalidInput, String::from("range out of bounds")));
        }
        if !self.ptr.is_null() {
            unsafe {
                intrinsics::msync(self.ptr.add(offset), len, MS_SYNC)
            }
        } else {
            Ok(())
        }
    }

    /// Makes the mapping read-only.
    pub fn make_read_only(self) -> IoResult<Mmap> {
        if self.ptr.is_null() {
            return Ok(Mmap { ptr: core::ptr::null(), len: 0 });
        }
        intrinsics::mprotect(self.ptr, self.len, PROT_READ)?;
        let ptr = self.ptr as *const u8;
        let len = self.len;
        core::mem::forget(self);
        Ok(Mmap { ptr, len })
    }
}

impl Drop for MmapMut {
    fn drop(mut self) {
        if !self.ptr.is_null() {
            let _ = intrinsics::munmap(self.ptr, self.len);
        }
    }
}

unsafe impl Send for MmapMut {}
unsafe impl Sync for MmapMut {}

/// Advice to the kernel about memory mapping access patterns.
pub enum MmapAdvice {
    Normal,
    Sequential,
    Random,
    WillNeed,
    DontNeed,
}

impl MmapAdvice {
    fn to_raw(self) -> i32 {
        match self {
            MmapAdvice::Normal     => MADV_NORMAL,
            MmapAdvice::Sequential => MADV_SEQUENTIAL,
            MmapAdvice::Random     => MADV_RANDOM,
            MmapAdvice::WillNeed   => MADV_WILLNEED,
            MmapAdvice::DontNeed   => MADV_DONTNEED,
        }
    }
}

// Platform constants
const PROT_READ: i32    = 0x1;
const PROT_WRITE: i32   = 0x2;
const MAP_SHARED: i32   = 0x01;
const MAP_PRIVATE: i32  = 0x02;
const MAP_ANONYMOUS: i32 = 0x20;
const MS_ASYNC: i32  = 1;
const MS_SYNC: i32   = 4;
const MADV_NORMAL: i32     = 0;
const MADV_RANDOM: i32     = 1;
const MADV_SEQUENTIAL: i32 = 2;
const MADV_WILLNEED: i32   = 3;
const MADV_DONTNEED: i32   = 4;
