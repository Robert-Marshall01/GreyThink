# Citation

If you use the `google/flan-t5-base` model in your work, please cite the model authors and the model page. A suggested minimal acknowledgement follows:

"This work uses the pre-trained model `google/flan-t5-base` (Apache License 2.0). Model available at: [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)"

For academic citation, please consult the model page for the authors and papers associated with the model and cite those original works as appropriate.

License information and required notices are included in `models/COPYING.APACHE2.txt` and `models/google-flan-t5-base.NOTICE.txt`.

## BibTeX

```bibtex
@misc{chung2022scaling,
  doi = {10.48550/ARXIV.2210.11416},
  url = {https://arxiv.org/abs/2210.11416},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  title = {Scaling Instruction-Finetuned Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
```
